# Talk2Data - Project Overview

## ğŸ¯ Vision
Talk2Data ist ein KI-gestÃ¼tztes Natural Language to SQL System, das es Business-Usern ermÃ¶glicht, Datenanalysen durch einfache Fragen in natÃ¼rlicher Sprache durchzufÃ¼hren - ohne SQL-Kenntnisse.

## ğŸ“Š Was macht Talk2Data?

**Problem:** Business-Analysten benÃ¶tigen SQL-Kenntnisse oder Data-Team-Support fÃ¼r Datenabfragen.

**LÃ¶sung:** 
- User stellt Frage auf Deutsch/Englisch: *"Zeige Umsatz nach Store im Januar 2024"*
- KI generiert automatisch: `SELECT store_name, SUM(sales) FROM ... WHERE date >= '2024-01-01'`
- System fÃ¼hrt Query aus und visualisiert Ergebnisse

## ğŸ—ï¸ Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Streamlit UI   â”‚ â† User Interface (Login, Fragen stellen, Schemas verwalten)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ HTTPS + JWT
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI       â”‚ â† REST API Backend
â”‚   /generate-sql â”‚ â† SQL Generation Endpoint
â”‚   /execute-sql  â”‚ â† Query Execution (geplant von Mo)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â–¼          â–¼         â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAI â”‚ â”‚ AWS â”‚  â”‚  AWS â”‚  â”‚   DB   â”‚
â”‚GPT-4o  â”‚ â”‚ S3  â”‚  â”‚Cognitoâ”‚ â”‚Connectorâ”‚
â”‚  mini  â”‚ â”‚     â”‚  â”‚       â”‚ â”‚(Mo's)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Features

### 1. Multi-Table SQL Generation (90% komplett)
- **LLM Table Selection:** KI wÃ¤hlt relevante Tabellen basierend auf Frage
- **Algorithmische JOINs:** System generiert automatisch korrekte JOIN-Pfade
- **Star Schema Support:** Optimiert fÃ¼r Fact + Dimension Tables
- **Date Intelligence:** Erkennt "Januar 2024" und konvertiert zu ISO-Format

**Beispiel:**
```
Frage: "Umsatz nach Store und Produkt"
â†’ WÃ¤hlt: fact_sales, dim_store, dim_product
â†’ Generiert JOINs automatisch
â†’ Erstellt SQL: SELECT store_name, product_name, SUM(sales) FROM fact_sales LEFT JOIN ...
```

### 2. Multi-Tenant Architecture (100% komplett)
- **User Isolation:** Jeder User hat eigene Schemas in S3
- **JWT Authentication:** AWS Cognito basierte Authentifizierung
- **Username Normalization:** Email â†’ Prefix Extraktion fÃ¼r S3-Pfade
- **S3 Storage:** `s3://bucket/schemas/{username}/{schema_name}.json`

### 3. Security Validator (100% komplett)
- **SQL Injection Protection:** Blockiert `OR 1=1`, `UNION SELECT`, etc.
- **Destructive Command Prevention:** Keine `DROP`, `DELETE`, `INSERT`
- **Comment Detection:** Blockiert `--` und `/* */` Patterns
- **Function Whitelist:** Nur erlaubte Aggregationen (SUM, AVG, COUNT...)

### 4. Schema Management (90% komplett)
- **Visual Schema Builder:** Streamlit UI zum Erstellen von Star Schemas
- **S3 CRUD Operations:** Create, Read, Update, Delete Schemas
- **Schema Validation:** PrÃ¼ft Format und Struktur
- **Local Fallback:** Nutzt lokale Schemas wenn S3 nicht verfÃ¼gbar

### 5. Database Connectors (20% komplett - Mo's Task)
- **PostgreSQL Connector:** Geplant von Mo
- **MySQL Connector:** Geplant von Mo
- **Result Visualization:** Charts und Tabellen (geplant von Mo)
- **/execute-sql Endpoint:** Query AusfÃ¼hrung (geplant von Mo)

## ğŸ”„ User Flow

### Flow 1: SQL Generieren
```
1. User meldet sich an (Cognito)
   â†“
2. WÃ¤hlt Schema aus (von S3 oder lokal)
   â†“
3. Stellt Frage: "Zeige Top 5 Stores"
   â†“
4. System extrahiert Username aus JWT Token
   â†“
5. LÃ¤dt User's Schema von S3
   â†“
6. LLM wÃ¤hlt relevante Tabellen (fact_sales, dim_store)
   â†“
7. Algorithmus generiert JOINs
   â†“
8. LLM erstellt finalen SQL Query
   â†“
9. Security Validator prÃ¼ft SQL
   â†“
10. SQL wird zurÃ¼ckgegeben (+ Confidence Score)
```

### Flow 2: Schema Erstellen
```
1. User Ã¶ffnet Schema Builder
   â†“
2. Definiert Fact Table (z.B. fact_sales)
   â†“
3. Definiert Dimension Tables (dim_store, dim_product)
   â†“
4. Definiert Relationships (store_id, product_id)
   â†“
5. Definiert KPIs (Umsatz, Gewinn, Marge)
   â†“
6. Speichert Schema zu S3: schemas/{username}/my_schema.json
```

## ğŸ§© Technologie Stack

### Backend
- **FastAPI:** REST API Framework
- **OpenAI GPT-4o-mini:** LLM fÃ¼r Table Selection & SQL Generation
- **Pydantic:** Data Validation
- **Boto3:** AWS SDK (S3, Cognito)
- **PyJWT:** Token Verification

### Frontend
- **Streamlit:** Web UI Framework
- **Requests:** HTTP Client fÃ¼r API Calls

### Infrastructure
- **Railway:** Cloud Hosting Platform
- **AWS S3:** Schema Storage
- **AWS Cognito:** User Management & Authentication
- **Docker:** Containerization

### Database (Mo's Part)
- **PostgreSQL:** Primary Database (geplant)
- **MySQL:** Alternative Database (geplant)
- **SQLAlchemy:** ORM Layer (geplant)

## ğŸ“ Projekt Struktur

```
talk2data/
â”œâ”€â”€ api_service.py              # FastAPI Backend (100% komplett)
â”œâ”€â”€ streamlit_app_auth.py       # Streamlit UI mit Auth (100% komplett)
â”œâ”€â”€ streamlit_schema_builder.py # Schema Builder UI (90% komplett)
â”œâ”€â”€ auth_service.py             # Cognito Integration (100% komplett)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ llm_sql_generator.py    # SQL Generation Engine (95% komplett)
â”‚   â”œâ”€â”€ schema_parser.py        # Schema Parser + JOIN Engine (100% komplett)
â”‚   â”œâ”€â”€ llm_table_selector.py  # Table Selection LLM (100% komplett)
â”‚   â”œâ”€â”€ sql_validator.py        # Security Validator (100% komplett)
â”‚   â”œâ”€â”€ s3_service.py           # S3 CRUD Operations (100% komplett)
â”‚   â”œâ”€â”€ date_converter.py       # Date Intelligence (100% komplett)
â”‚   â”œâ”€â”€ mapping.py              # KPI Mappings (80% komplett)
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                 # Lokale Schema Dateien
â”‚   â”‚   â”œâ”€â”€ retial_star_schema.json  # Main Test Schema
â”‚   â”‚   â”œâ”€â”€ rossman_schema.json      # Legacy Schema
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ db_connector.py         # Database Connector (0% - Mo's Task)
â”‚       postgres_connector.py   # PostgreSQL (0% - Mo's Task)
â”‚       mysql_connector.py      # MySQL (0% - Mo's Task)
â”‚
â”œâ”€â”€ prompts/                    # LLM Prompt Templates
â”‚   â”œâ”€â”€ sql_generator_system.txt
â”‚   â”œâ”€â”€ sql_generator_user.txt
â”‚   â”œâ”€â”€ table_selector.txt
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ docs/                       # Diese Dokumentation
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md
â”‚   â”œâ”€â”€ TECHNICAL_DOCUMENTATION.md
â”‚   â”œâ”€â”€ API_DOCUMENTATION.md
â”‚   â””â”€â”€ DEPLOYMENT_GUIDE.md
â”‚
â”œâ”€â”€ Dockerfile                  # API Container
â”œâ”€â”€ Dockerfile.streamlit        # Streamlit Container
â”œâ”€â”€ requirements.txt            # Python Dependencies
â””â”€â”€ .env                        # Environment Variables
```

## ğŸ‘¥ Team & Verantwortlichkeiten

### Raed's Arbeit (80% des Projekts)
âœ… **Komplett:**
- FastAPI Backend Architecture
- Multi-Table SQL Generation mit LLM
- Algorithmischer JOIN Generator
- AWS Cognito Authentication
- Multi-Tenant S3 Integration
- Security Validator
- Schema Parser & Management
- Date Converter
- Streamlit UI (Authentication, Schema Builder, Query Interface)
- Railway Deployment

â³ **In Arbeit:**
- Business KPIs erweitern (Profit, Margin, Growth)
- Schema Format Konsistenz
- Test-Szenarien definieren

### Mo's Arbeit (20% des Projekts)
â³ **Geplant:**
- Database Connectors (PostgreSQL, MySQL)
- `/execute-sql` Endpoint
- Result Visualization (Charts, Tabellen)
- Query Performance Monitoring
- Testing der Connectors

## ğŸ“Š Projektstatus

| Komponente | Status | Details |
|-----------|--------|---------|
| Multi-Table SQL | âœ… 95% | JOIN Generation funktioniert |
| Multi-Tenant | âœ… 100% | S3 + JWT komplett |
| Security | âœ… 100% | Validator aktiv |
| Schema Management | âœ… 90% | CRUD funktioniert |
| Authentication | âœ… 100% | Cognito integriert |
| Database Execution | â³ 0% | Mo's Task |
| Visualization | â³ 0% | Mo's Task |

**Gesamtfortschritt:** ~80% komplett

## ğŸ¯ NÃ¤chste Schritte

### Phase 0 (aktuell - 1 Woche)
1. âœ… Multi-Tenant S3 aktivieren
2. âœ… SQL Validator aktivieren
3. â³ Business KPIs erweitern (Raed)
4. â³ Database Connectors implementieren (Mo)

### Phase 1 (Woche 2-3)
1. `/execute-sql` Endpoint (Mo)
2. Result Visualization (Mo)
3. Query History & Caching
4. Performance Optimization

### Phase 2 (Woche 4+)
1. Advanced KPIs (YoY, MoM Growth)
2. Multi-Database Support
3. Query Optimization Suggestions
4. Admin Dashboard

## ğŸš€ Live System

**API:** https://talk2data-production.up.railway.app
**Streamlit:** (wird noch deployed)

**Test Credentials:** (AWS Cognito)
- Username: `testuser`
- Password: [aus .env]

## ğŸ“ Wichtige Entscheidungen

### Warum OpenAI statt lokales LLM?
- **QualitÃ¤t:** GPT-4o-mini generiert sehr gute SQL Queries
- **Geschwindigkeit:** API ist schneller als lokale Modelle
- **Kosten:** ~$0.001 pro Query (sehr gÃ¼nstig)
- **Wartung:** Kein Model Training/Hosting nÃ¶tig

### Warum Star Schema?
- **BI Standard:** Fact + Dimensions ist Industrie-Standard
- **JOIN Algorithmus:** Eindeutige Paths von Fact zu Dimensions
- **Performance:** Optimiert fÃ¼r analytische Queries
- **VerstÃ¤ndlichkeit:** Business Users kennen die Struktur

### Warum Security Validator ohne Schema-Checks?
- **LLM Verantwortung:** LLM kennt Schema und wÃ¤hlt valide Tables/Columns
- **Security First:** Injection-Prevention ist kritischer
- **FlexibilitÃ¤t:** Funktioniert mit allen Schema-Formaten
- **Einfachheit:** Weniger false positives

## ğŸ“ Kontakt

**Raed Mokdad**
- Email: raed.mokdad@example.com
- GitHub: raedmokdad/talk2data

**Mo** (Database Spezialist)
- Verantwortlich fÃ¼r: DB Connectors, Query Execution, Visualization
